# EES performance tests

The performance test suite is built using [k6](https://k6.io/) and visualised using
[Grafana](https://grafana.com/) and [InfluxDB](https://www.influxdata.com/).

## Quick start

1. Start your local EES environment.
2. Ensure you have `127.0.0.1  ees.local` in your `hosts` file.
3. If running in Ubuntu, refer to the [Ubuntu firewall steps](#ubuntu-firewall-steps).
4. Run:
    ```bash
    cd tests/performance-tests
   
    # Install Node dependencies.
    pnpm i
   
    # Copy the default .env JSON file for a local environment.
    cp .env.example.json .env.local.json  
    
    # Start Grafana and InfluxDB.
    pnpm start
   
    # Compile the tests and copy .env.local.json to dist/ folder.
    pnpm webpack
   
    # Obtain access tokens for the "bau1" user.
    pnpm --environment=local --users=bau1 auth
    
    # Run the import test.
    pnpm --environment=local perftest dist/import.test.js
    ```
5. This example will run the import test against your local environment.
   Navigate to the [EES Grafana Dashboard](http://localhost:3005/d/ees-dashboard/ees-dashboard?orgId=1&refresh=5s) and
   expand the "Imports" panel to watch the results.
6. You can stop the test at any time and clean up the test data generated by running:
    ```bash
    # Clear down the performance test data.
    pnpm --environment=local perftest dist/cleanUp.test.js
   
    # Stop Grafana and InfluxDb.
    pnpm stop
    ```

## How it works

* We run InfluxDB (for collecting data) and Grafana (for visualising the collected data) using Docker Compose.
* We run the K6 tests using Docker Compose. We can choose specific tests to run from the CLI.
* The K6 tests run and post data to InfluxDB.
* Grafana is set up with a Dashboard that consumes and visualises data from the InfluxDB data source.

## Installation

### Requirements

You will need the following dependencies to run the tests successfully:

- [Docker and Docker Compose](https://docs.docker.com/)
- [NodeJS v16+](https://nodejs.org/)

## Running the tests

NOTE: All commands in this README are issued from the `tests/performance-tests` folder.

### Install dependencies with NPM

Run:

```bash
pnpm i
```

### Add ees.local to your hosts file

This step is only necessary if running tests against the host machine. Add the following line to
your hosts file:

```
127.0.0.1    ees.local
```

### Start InfluxDB and Grafana

Note that this step isn't strictly necessary if we're not wanting to record metrics whilst
generating load - the tests can run independently of these if we're just wanting to generate
some load, for example.

```bash
pnpm start
```

To stop them later, run `pnpm stop`. Note that this will not destroy any data. To remove any data,
run `pnpm stop:clear`.

### Compile the tests

The tests are written in Typescript so we need to transpile them to work in K6 (which as an aside is
Go-based, not Node JS-based). We use Webpack and Babel for this.

```bash
pnpm webpack 
```

or for continual compilation during development:

```bash
pnpm webpack watch
```

### Run the tests

#### Create environment-specific .env.<environment>.json files

This step is only required if running performance tests that require access to the Admin API.
These files will store environment-specific user credentials for accessing Admin.

As a one-off, we will need to copy `tests/performance-tests/.env.example.json` to
`tests/performance-tests/.env.<environment>.json`, and supply the file with the correct
environment-specific credentials for the users that we'll be using. The `<environment>` can
be any value that we can load test against. As an example, if doing load testing or test script
development against a local environment, we would create a file called
`tests/performance-tests/.env.local.json` and supply the local user credentials within the file.

<a id="ubuntu-firewall-steps"></a>
#### Allow access to host ports from containers

This step is only required if running performance tests locally against the host machine.

If running Ubuntu and running the tests against your local machine, ports under test from K6
will be protected by default by `Ubuntu Firewall`.

To grant access to these ports from containers on the
[K6 subnet as defined in docker-compose.yml](docker-compose.yml),
run the following commands:

```bash
sudo ufw allow from 172.30.0.0/24 to any port 3000 # Public site
sudo ufw allow from 172.30.0.0/24 to any port 5000 # Data API
sudo ufw allow from 172.30.0.0/24 to any port 5010 # Content API
sudo ufw allow from 172.30.0.0/24 to any port 5021 # Admin site / Admin API
sudo ufw allow from 172.30.0.0/24 to any port 5031 # Keycloak
sudo ufw allow from 172.30.0.0/24 to any port 5051 # Public API
```

#### Obtain auth tokens for Admin testing and data creation during test setup

This step is only required if running performance tests that require access to the Admin API.
It requires the above step of creating environment-specific .env.<environment>.json files first.

```bash
pnpm --environment=<environment> --users=<user names> auth
``` 

This obtains an `access_token` and a `refresh_token` that can be used to access protected resources
in the Admin API. The `refresh_token` allows long-running tests to refresh their access token if
it's going to expire mid-test.

This will look in the `.env.<environment>.json` file for a user with `"name": "<user name>"` and use
that user's credentials to log into Admin in order to obtain their auth tokens.

Details of the environment and the users' access tokens can then be found in a generated file
named `dist/.auth-tokens.<environment>.json`. This file is then used by the tests
themselves to run against the same environment.

As a concrete example:

```bash
pnpm --environment=local --users=bau1 auth
```

will store access and refresh tokens for the user `bau1` against the local environment.

#### Run individual tests

```bash
pnpm --environment=<environment> perftest dist/some-test-script.test.js
```

An example of running an actual script would be:

```bash
pnpm --environment=local perftest dist/import.test.js
```

Each test script is runnable against any environment. They will find existing or set up new
dependent data before the test's VUs begin running, and will tear down any data that cannot be
reused by a subsequent run when the tests finish. This is achieved using K6's `setup()` and
`teardown()` lifecycles.

### Monitor the test results

View the results of real-time or historic test runs by visiting:

```
http://localhost:3005/d/ees-dashboard/ees-dashboard?orgId=1&refresh=5s
```

This Dashboard shows EES-specific custom metrics, such as the length of time it takes to
complete table tool queries, the stages of progress that importing data files have reached
so far, and so on.

Also we can visit:

```
http://localhost:3005/d/k6/k6-load-testing-results?orgId=1&refresh=5s
```

This is an out-of-the-box Grafana / K6 Dashboard that captures general low-level performance
statistics.

## Command-line test parameters

Some variables are available in certain tests to allow the running of the tests with different
test data should we need to do so, but without needing to alter the test code. We use environment
variables to supply the tests with variables using the `-e` flag. All variables have default
values to fall back on.

Note that in various tests that deal with file imports, we allow the selection of data files to
use with that test on the command line. We can supply large data files as ZIP files using the naming
convention of `big-file1.zip`, `big-file2.zip` etc, and place them in the
[imports assets folder](src/tests/admin/import/assets). With this naming convention, they will
be ignored by Git.

### Common load profiles

Tests which use the [common configuration generators](src/configuration) can have their configuration
fine-tuned using the following parameters.

#### Steady request rates

See [steadyRequestRateProfile.ts](src/configuration/steadyRequestRateProfile.ts).

This is typically used for load and soak tests, where a steady volume of traffic is maintained
for a given duration.

* RPS - the rate of requests generated per second.
* MAIN_TEST_STAGE_DURATION_MINS - the duration of the main stage of the test. Also the duration of a
  subsequent cooldown period which allows in-flight requests and responses to finish.

Example for running at a steady 15 RPS for 20 minutes, followed by a 20 minute cooldown period:

```
pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js \
  -e PROFILE=load \
  -e MAIN_TEST_STAGE_DURATION_MINS=20 \
  -e RPS=15
```

#### Ramping request rates

See [rampingRequestRateProfile.ts](src/configuration/rampingRequestRateProfile.ts).

This is typically used for stress testing, where traffic starts from zero requests per minute and
slowly increases over time, to find the point at which the system under test becomes unstable.

* RPS - the rate of requests generated per second at the point of maximum stress (the end of the main
  test stage).
* RAMPING_TEST_STAGE_DURATION_MINS - the duration of the period during which the test ramps up from 
  zero RPS to the maximum RPS.
  subsequent cooldown period which allows in-flight requests and responses to finish.
* MAIN_TEST_STAGE_DURATION_MINS - a duration after the ramping up period to maintain the maximum RPS.
  Also the duration of a subsequent cooldown period which allows in-flight requests and responses to
  finish.

Example for ramping up from zero to 30 RPS maximum over 10 minutes, with a further 15 minutes maintaining
the maximum RPS and then a final 15 minute cooldown period:

```
pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js \
  -e PROFILE=stress \
  -e RAMPING_TEST_STAGE_DURATION_MINS=10 \
  -e MAIN_TEST_STAGE_DURATION_MINS=15 \
  -e RPS=30
```

#### Spike

See [spikeProfile.ts](src/configuration/spikeProfile.ts).

This is typically used to test sudden spikes in traffic, the immediate effect on the system under test
and the recovery time post-spike.

* PRE_SPIKE_DURATION_MINS - the duration of the stage prior to the traffic spike.
* SPIKE_DURATION_MINS - the duration of the traffic spike.
* POST_SPIKE_DURATION_MINS - the duration of the recovery stage after the traffic spike.
* RPS_NORMAL - the rate of requests generated per second under normal traffic conditions.
* RPS_SPIKE - the rate of requests generated per second during the traffic spike period.

Example of testing normal traffic at 10 RPS for 5 minutes, a 1 minute spike at 50 RPS, and then
a 10 minute period of normal traffic following the spike:

```
pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js \
  -e PROFILE=spike \
  -e PRE_SPIKE_DURATION_MINS=5 \
  -e RPS_NORMAL=10 \
  -e SPIKE_DURATION_MINS=1
  -e RPS_SPIKE=50
  -e POST_SPIKE_DURATION_MINS=10
```

#### Sequential executions

See [sequentialRequestsProfile.ts](src/configuration/sequentialRequestsProfile.ts).

This is used to execute the main test script one at a time, with no concurrency. This is
typically used to be able to measure performance on an individual request basis.

* MAIN_TEST_STAGE_DURATION_MINS - the duration of the main stage of the test. There is no
  cooldown period with this profile.

Example of running a test over a 15 minute duration, executing the tests one after another
with no parallelisation.  Each test execution begins immediately after the previous one 
completes. 

```
pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js \
  -e PROFILE=sequential \
  -e MAIN_TEST_STAGE_DURATION_MINS=15
```

### Individual test options

Full sets of options per test are available below as examples:

#### import.test.js

* PUBLICATION_TITLE - default value is "import.test.ts".
* DATA_FILE - default value is "small-file.csv" which is in source control. See notes above on the use
  of large ZIP files.

`pnpm --environment=dev perftest dist/import.test.js
-e PUBLICATION_TITLE="Import publication" -e DATA_FILE="big-file1.zip"`

#### getReleasePage.test.js

##### Profiles

This test supports various different performance testing scenarios.

* PROFILE - supported values are "load", "stress", "spike", "sequential".

Each of these profiles has a default set of configuration out-of-the-box. They can however be
fine-tuned further using the common override parameters defined in
[Common load profiles](#common-load-profiles).

#### publicTableBuilderQuery.test.js

* PUBLICATION_TITLE - default value is "publicTableBuilderQuery.test.ts".
* DATA_FILE - default value is "small-file.csv" which is in source control. See notes above on the use
  of large ZIP files.

`pnpm --environment=dev perftest dist/publicTableBuilderQuery.test.js
-e PUBLICATION_TITLE="Public table builder query" -e DATA_FILE="big-file1.zip"`

#### adminTableBuilderQuery.test.js

* PUBLICATION_TITLE - default value is "adminTableBuilderQuery.test.ts".
* DATA_FILE - default value is "small-file.csv" which is in source control. See notes above on the use
  of large ZIP files.

`pnpm --environment=dev perftest dist/adminTableBuilderQuery.test.js
-e PUBLICATION_TITLE="Admin table builder query" -e DATA_FILE="big-file1.zip"`

#### publicApiDataSetQuery.test.js

##### Profiles

This test supports various different performance testing scenarios.

* PROFILE - supported values are "load", "stress", "spike", "sequential".

Each of these profiles has a default set of configuration out-of-the-box. They can however be
fine-tuned further using the common override parameters defined in
[Common load profiles](#common-load-profiles).

##### Query generation

There are 2 high-level modes for running these tests.  The first is by running a fixed set of known queries,
and the second is via random query generation based upon the published data sets in the environment.

###### Fixed query mode

In this query mode, predefined queries are selected by filename, and the test will then limit the queries called 
to just these selected ones.  This is supported by using the following parameter:

* QUERY_FILES - a comma-separated list of file paths.

  `pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js
  -e QUERY_FILES=dev/api-tests-absence-query1.json,dev/api-tests-absence-query2.json`

Fixed query files can be generated by hand, or by enabling query logging whilst running in random mode and inspecting 
the output.  The following example targets a particular published data set and logs out the randomly generated queries. 

```bash
pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js \
  -e PROFILE=sequential \
  -e DATA_SET_TITLES_TO_INCLUDE="Performance tests - API test absence" \ 
  -e LOG_QUERIES=true
```

This outputs queries in the format:

```
{
  "publicationTitle": "Fixed Data Set Queries",
  "dataSetName": "Performance tests - API test absence",
  "dataSetId": "06949201-f362-4d75-90a7-40625a3e35aa",
  "query": {...}
}
```

which can then be copied directly into a new file within the 
[fixed-queries](src/tests/public-api/data-set-query/fixed-queries) folder.

###### Random query mode

In this query mode, random queries are generated based upon data sets that are published in the environment under 
test.  The tests firstly discover all published data sets, then applies any filtering based upon available command-line
parameters to identify candidate data sets for the test run.  Thereafter, the tests generate random queries based upon
the data set metadata available.

A number of parameters can be used to limit the candidate data sets to suit the scenarios being tested:

* DATA_SET_TITLES_TO_INCLUDE - a comma-separated list of data set titles, which will cause the test to only use those
  data sets during the run. The default value is undefined, which does not filter data sets by title.

  `pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js
  -e DATA_SET_TITLES_TO_INCLUDE="Data Set 1"`

* DATA_SET_TITLES_TO_EXCLUDE - a comma-separated list of data set titles, which will cause the test to avoid using those
  data sets during the run. The default value is undefined, which does not filter out any data sets by title.

  `pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js
  -e DATA_SET_TITLES_TO_EXCLUDE="Data Set 1"`  

* DATA_SET_MAX_ROWS - the maximum number of rows for data sets to be used in this run. The default value
  is undefined, which does not filter data sets by their size.

  `pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js
  -e DATA_SET_MAX_ROWS=1000000`

Additionally, generated queries can be simple or complex.  Simple queries use a limited number of available facets 
(e.g. filters), limit operator usage to `eq` and `in`, and limit query nesting to 1 level.  Complex queries use greater 
numbers of facets, all available operators and deeper nesting.  

This can be controlled with the following parameter: 

* QUERY_COMPLEXITY - default value is "simple", which generates simple small queries. Other value is "complex"
  which generates nested queries with more complex operators.

  `pnpm --environment=dev perftest dist/publicApiDataSetQuery.test.js
  -e QUERY_COMPLEXITY=complex`

## Transpiling and Bundling

The tests are written in Typescript so we need to transpile them to work in K6 (which as an aside is
Go-based, not Node JS-based). By default, k6 can only run ES5.1 JavaScript code. To use TypeScript, we
set up a bundler that converts TypeScript to JavaScript code.

If you want to learn more, check out
[Bundling node modules in k6](https://k6.io/docs/using-k6/modules#bundling-node-modules).

We also have some Typescript Node scripts that are used directly by Node.js that require bundling and
transpiling in the same fashion.

## Troubleshooting

### Exceptions and stacktraces in Typescript code

We can use source maps to be able to trace errors back to the original Typescript source when
encountering errors in transpiled Javascript code. This is provided via the `source-map-support`
package.

To enable this in Node, we can supply the `-r source-map-support/register` option whilst running the
problem script. For example:

```bash
node --enable-source-maps dist/logAuthTokens.js local bau1
```

would result in stacktraces that point back to the original `src/auth/logAuthTokens.ts` Typescript
file.

### Healthcheck tests

We have some test scripts that ascertain that the infrastructure of the environment we're testing
against and our own utils and helper scripts are behaving themselves.

#### Refresh token support

Run the following to test that the environment under test supports refresh tokens (as specified
in your `.env.<environment>.json file).

```bash
pnpm --environment=<environment> --users=bau1 auth
pnpm --environment=local perftest dist/refreshTokens.test.js
```

The output should look like:

```text
     ✓ response with original access token was 200
     ✓ response with refreshed tokens was successful
     ✓ response with refreshed tokens contained a new accessToken
     ✓ response with refreshed tokens contained a new refreshToken
     ✓ response with refreshed access token was 200
     ✓ response with re-refreshed tokens was successful
     ✓ response with re-refreshed tokens contained a new accessToken
     ✓ response with re-refreshed tokens contained a new refreshToken
     ✓ response with re-refreshed access token was 200
```

### Client-side errors when generating load

Errors can be encountered when generating load from a host machine due to insufficient resources. K6 has
a guide for [running large tests](https://k6.io/docs/testing-guides/running-large-tests/) which makes some
suggestions as to how to fine-tune a machine for load generation. The following settings changes can make
a big difference on a Linux machine:

As root:

```
sysctl -w net.ipv4.ip_local_port_range="1024 65535"
sysctl -w net.ipv4.tcp_tw_reuse=1
sysctl -w net.ipv4.tcp_timestamps=1
ulimit -n 250000
```
